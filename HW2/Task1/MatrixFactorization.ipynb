{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from collections import Counter\n",
    "import random\n",
    "from utils import create_dataset\n",
    "from utils import load_model, save_checkpoint\n",
    "from utils import AveragePrecision\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victai/.local/lib/python3.5/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "rating_data = pd.read_csv(\"data/rating_train.csv\")\n",
    "rating_data = rating_data.drop(['date'], axis=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cnt = {}\n",
    "for i in user_ids:\n",
    "    data_cnt[i] = len(rating_data[rating_data['userid'] == i])\n",
    "\n",
    "#plt.hist(list(data_cnt.values()), bins=20)\n",
    "#list(data_cnt.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_food_score = [{} for i in range(max(user_ids)+1)]\n",
    "for u, f in zip(rating_data['userid'], rating_data['foodid']):\n",
    "    if user_food_score[u].get(f) != None:\n",
    "        user_food_score[u][f] += 1\n",
    "    else:\n",
    "        user_food_score[u][f] = 1\n",
    "    \n",
    "for i in user_ids:\n",
    "    all_val = list(user_food_score[i].values())\n",
    "    for k, v in user_food_score[i].items():\n",
    "        user_food_score[i][k] = (v - min(all_val)) / (max(all_val) - min(all_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_percentage = 0.5\n",
    "user_id_train, user_id_valid, food_id_train, food_id_valid, Y_train, Y_valid = create_dataset(user_food_score, rating_data, valid_percentage)\n",
    "\n",
    "order = np.arange(len(user_id_train))\n",
    "random.seed(0)\n",
    "random.shuffle(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_eaten = {}\n",
    "for u in user_ids:\n",
    "    t_start_idx = np.where(user_id_train == u)[0][0]\n",
    "    t_end_idx = np.where(user_id_train == u)[0][-1]\n",
    "    v_start_idx = np.where(user_id_valid == u)[0][0]\n",
    "    v_end_idx = np.where(user_id_valid == u)[0][-1]\n",
    "    #tmp = []\n",
    "    #for i in range(v_start_idx, v_end_idx+1):\n",
    "        #if (food_id_valid[i] not in food_id_train[t_start_idx: t_end_idx+1]) and (food_id_valid[i] not in tmp):\n",
    "        #    tmp.append(food_id_valid[i])\n",
    "    #print(u)\n",
    "    #print(t_end_idx)\n",
    "    not_eaten[u] = np.setdiff1d(food_id_valid[v_start_idx: v_end_idx+1], food_id_train[t_start_idx: t_end_idx+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self, n_users, n_foods, n_dim):\n",
    "        super(MatrixFactorization, self).__init__()\n",
    "        self.user_embed_layer = nn.Embedding(n_users, n_dim).cuda()\n",
    "        self.food_embed_layer = nn.Embedding(n_foods, n_dim).cuda()\n",
    "        self.user_bias_embed_layer = nn.Embedding(n_users, 1).cuda()\n",
    "        self.food_bias_embed_layer = nn.Embedding(n_foods, 1).cuda()\n",
    "        \n",
    "    def forward(self, user_train, food_train):\n",
    "        user_vec = self.user_embed_layer(user_train.cuda())\n",
    "        user_vec = nn.Dropout(0.3)(user_vec)\n",
    "        user_vec = nn.Sigmoid()(user_vec)\n",
    "        food_vec = self.food_embed_layer(food_train.cuda())\n",
    "        food_vec = nn.Dropout(0.3)(food_vec)\n",
    "        food_vec = nn.Sigmoid()(food_vec)\n",
    "        user_bias = self.user_bias_embed_layer(user_train.cuda()).view(-1)\n",
    "        food_bias = self.food_bias_embed_layer(food_train.cuda()).view(-1)\n",
    "        \n",
    "        lamb = 0.01\n",
    "        reg1 = lamb * torch.norm(torch.cat([x.view(-1) for x in self.parameters()]), 1)\n",
    "        reg2 = lamb * torch.norm(torch.cat([x.view(-1) for x in self.parameters()]), 2)\n",
    "        \n",
    "        return ((user_vec * food_vec).sum(1) + user_bias + food_bias).cuda(), reg1+reg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 10\n",
    "model = MatrixFactorization(max(user_ids)+1, food_data.shape[0], n_dim).cuda()\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, train_loss = 0.578853964805603, valid_loss = 0.5194810628890991, best = 0.5194810628890991, best_map = 0\n",
      "max: 0.0, mean: 0.0\n",
      "Epoch 20, train_loss = 0.17961128056049347, valid_loss = 0.1657111942768097, best = 0.1657111942768097, best_map = 0\n",
      "Epoch 30, train_loss = 0.08602795004844666, valid_loss = 0.08283458650112152, best = 0.08283458650112152, best_map = 0\n",
      "max: 0.0, mean: 0.0\n",
      "Epoch 40, train_loss = 0.06296724081039429, valid_loss = 0.06226726621389389, best = 0.06226726621389389, best_map = 0\n",
      "Epoch 50, train_loss = 0.05498889461159706, valid_loss = 0.05483543500304222, best = 0.05483543500304222, best_map = 0\n",
      "max: 0.0, mean: 0.0\n",
      "Epoch 60, train_loss = 0.0508057177066803, valid_loss = 0.0508696585893631, best = 0.0508696585893631, best_map = 0\n",
      "Epoch 70, train_loss = 0.048136156052351, valid_loss = 0.04832153022289276, best = 0.04832153022289276, best_map = 0\n",
      "max: 0.0, mean: 0.0\n",
      "Epoch 80, train_loss = 0.04641120508313179, valid_loss = 0.04657445102930069, best = 0.04657445102930069, best_map = 0\n",
      "Epoch 90, train_loss = 0.045085083693265915, valid_loss = 0.045239128172397614, best = 0.045239128172397614, best_map = 0\n",
      "max: 0.0, mean: 0.0\n",
      "Epoch 100, train_loss = 0.04414656385779381, valid_loss = 0.04433167725801468, best = 0.04433167725801468, best_map = 0\n",
      "Epoch 110, train_loss = 0.043496035039424896, valid_loss = 0.04356062412261963, best = 0.04356062412261963, best_map = 0\n",
      "max: 0.0, mean: 0.0\n",
      "Epoch 120, train_loss = 0.042909443378448486, valid_loss = 0.04301707446575165, best = 0.04301707446575165, best_map = 0\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "batch_size = 512\n",
    "save_every = 10\n",
    "print_every = 10\n",
    "infer_every = 20\n",
    "best = 1e10\n",
    "best_epoch = 1\n",
    "best_map = 0\n",
    "for e in range(1, epochs+1):\n",
    "    if e - best_epoch > 10: break\n",
    "    t_loss = 0\n",
    "    for b in range(len(user_id_train) // batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        u_train = Variable(torch.from_numpy(user_id_train[b*batch_size: (b+1)*batch_size]).type(torch.LongTensor))\n",
    "        f_train = Variable(torch.from_numpy(food_id_train[b*batch_size: (b+1)*batch_size]).type(torch.LongTensor))\n",
    "        \n",
    "        pred, regu_loss = model(u_train, f_train).cuda()\n",
    "        loss = loss_func(pred, Variable(torch.from_numpy(Y_train[b*batch_size: (b+1)*batch_size]).float()).cuda())+regu_loss\n",
    "        t_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    t_loss /= (len(user_id_train) // batch_size)\n",
    "    t_loss = t_loss.cpu().data.numpy()\n",
    "\n",
    "    # validation\n",
    "    v_pred = model(Variable(torch.LongTensor(user_id_valid.tolist())), Variable(torch.LongTensor(food_id_valid.tolist()))).cuda()\n",
    "    v_loss = loss_func(v_pred, Variable(torch.from_numpy(Y_valid).float()).cuda()).cpu().data.numpy()\n",
    "    \n",
    "    if e % infer_every == 0:\n",
    "        all_ap, _ = infer()\n",
    "        max_ap = np.array(all_ap).max()\n",
    "        mean_ap = np.array(all_ap).mean()\n",
    "        print(\"max: {}, mean: {}\".format(max_ap, mean_ap))\n",
    "        if mean_ap > best_map:\n",
    "            best_map = mean_ap\n",
    "    \n",
    "    if e % save_every == 0:\n",
    "        save_checkpoint({\n",
    "            'epoch': e,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'val_best': best\n",
    "            },\n",
    "            False,\n",
    "            'model.h5'\n",
    "        )\n",
    "    if v_loss < best:\n",
    "        save_checkpoint({\n",
    "            'epoch': e,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'val_best': v_loss\n",
    "            },\n",
    "            True,\n",
    "            'model.h5'\n",
    "        )\n",
    "        best = v_loss\n",
    "        best_epoch = e\n",
    "    \n",
    "    if e % print_every == 0:\n",
    "        print(\"Epoch {}, train_loss = {}, valid_loss = {}, best = {}, best_map = {}\".format(e, t_loss, v_loss, best, best_map))\n",
    "print(\"best_epoch: {}, best_valid_loss: {}, best_map: {}\".format(best_epoch, best, best_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint model.h5.best\n",
      "=> loaded checkpoint model.h5.best (epoch: 252)\n"
     ]
    }
   ],
   "source": [
    "model,_,_,_ = load_model('model.h5.best', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(): \n",
    "    F = np.arange(len(food_data), dtype=int)\n",
    "    F = [i for i in range(len(food_data))]\n",
    "    all_ans = []\n",
    "    all_score = []\n",
    "    for u in user_ids:\n",
    "        ans = []\n",
    "        U = [u for i in range(len(food_data))]\n",
    "        res = model(Variable(torch.LongTensor(U)), Variable(torch.LongTensor(F))).cpu().data.numpy()\n",
    "        res = res.flatten()\n",
    "        D = {k: v for (k, v) in zip(F, res)}\n",
    "        sorted_D = sorted(D.items(), key=lambda d: d[1], reverse=True)\n",
    "        for x in sorted_D:\n",
    "            if len(ans) >= 20: break\n",
    "            if x[0] in user_food_score[u].keys():\n",
    "                continue\n",
    "            ans.append(x[0])\n",
    "        all_score.append(AveragePrecision(not_eaten[u], ans, len(ans)))\n",
    "        all_ans.append(ans) \n",
    "    return all_score, all_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134,\n",
       "       135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147,\n",
       "       148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160,\n",
       "       161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173,\n",
       "       174, 175])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_eaten[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pred.csv\", 'w') as f:\n",
    "    f.write(\"userid,foodid\\n\")\n",
    "    for i in range(len(user_ids)):\n",
    "        f.write(\"{},\".format(user_ids[i]))\n",
    "        for j in all_ans[i]:\n",
    "            f.write(\"{} \".format(j))\n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
